---
layout: post
title: "Building a Kubernetes Cluster on a Raspberry Pi Official Way"
categories: blog
author_picture: https://avatars3.githubusercontent.com/tomjenningss
author_github: https://github.com/tomjenningss
seo-title: Building a Kubernetes Cluster on a Raspberry Pi - OpenLiberty.io
seo-description: Building a Kubernetes Cluster on a Raspberry Pi.
blog_description: "MicroProfile Reactive Messaging provides an easy way to send and receive messages within and between microservices using Kafka message brokers. Also introduces easier testing of your data source connections in Liberty apps with REST APIs, and some updates to OpenID Connect Server."
---
= Building a Kubernetes Cluster on a Raspberry Pi: Official Way in 20 Steps
Tom Jennings <https://github.com/tomjenningss>

image::/img/blog/Kube-cluster/pi_cluster.png[align="center"]

This is concise blog on how to create a Kubernetes cluster in the correct order. With the overall aim to demonstrate how cool the concept of using Kubernetes is, and deploying an actual app onto it.

Aimed at Beginners who know very little about Raspberry Pi's and Kubernetes to trained professionals in the industry who are wanting to create a Kubernetes cluster on the Raspberry Pi.


The goal of this Blog is to simplify and contextualise the creation of a Kubernetes cluster. In this guide it is going to tell you what hardware, software and commands you are going to use to set up and configure a Kubernetes cluster in the correct order. 

This is the first of three blog posts, all of which are seperate entities to one another, so you dont have to do step two to do step three, as the images are on Github ready to download, however if you wanted to do all three parts to gain the skills about re-deploying a Docker Image onto ARM architecture and then onto a Kubernetes cluster, the option is there. There are a lot of blogs out there which will tell you to run commands, and use flags you may not have heard of before. So I have explained how to create a Kubernetes cluster and what is being done.

Once we have created a Kubernetes cluster, we are going to deploy a light weight application onto the Kubernetes cluster which will demonstrate how kubernetes works and prove the concept that this open source technology is beneficial to use, and why it is beneficial to use in a production environment.


== Deploying Docker Images onto ARM Architecture

Once the Kubernetes cluster is created we can build Docker Images on ARM architecture. Raspberry Pi uses a RISC (Reduced Instruct Set Computing), where as intel uses x86 architecture which is CISC, (Complex Instruct Set Computing), so pulling down the Docker images for Liberty Bikes are compiled with x86 architecture, meaning that it will not run on Raspberry Pi. 

//Follow the link to see how to compile and deploy Docker Images onto a Kubernetes Cluster 

== Deploying Liberty Bikes onto Kubernetes cluster

image::/img/blog/Kube-cluster/liberty-bikes.png[align="center"]


Take a quick look at link:/https://github.com/OpenLiberty/liberty-bikes[Liberty Bikes] here and deploy it locally, on x86 to test it out before deploying it onto you Kubernetes cluster.

== Contents:



* <<prereq,Prerequisites>>
* <<KandC,Understanding Kubernetes and Concepts>>
* <<KCoRPI,Creating a Kubernetes cluster on a Raspberry Pi>>
* <<PE,Benefits of using Kubernetes in a production environment>>


[#prereq]
== Prerequesits

=== Hardware:

* Four link:/https://www.amazon.co.uk/Raspberry-Pi-3-Model-B/dp/B07BDR5PDW/ref=sr_1_3?keywords=raspberry+pi+3b%2B&qid=1568804412&s=gateway&sr=8-3[ Raspberry Pi 3 Model B+]


* Two link:/https://www.amazon.co.uk/Anker-PowerCore-20100-20000mAh-Technology-Black/dp/B00VJSGT2A/ref=sr_1_5?crid=37HTN71P9O4DJ&keywords=anker+power+bank&qid=1568804550&s=gateway&sprefix=Anker+%2Caps%2C144&sr=8-5[Anker Ultra High Capacity Power Bank]


* Four link:/https://www.amazon.co.uk/Anker-PowerCore-20100-20000mAh-Technology-Black/dp/B00VJSGT2A/ref=sr_1_5?crid=37HTN71P9O4DJ&keywords=anker+power+bank&qid=1568804550&s=gateway&sprefix=Anker+%2Caps%2C144&sr=8-5[32GB MicroSD Cards]

* Four link:/https://www.amazon.co.uk/CABEPOW-Braided-Android-Charging-Motorola-3Pack-3M/dp/B07L1HDW4P/ref=sr_1_3?keywords=4+pack+red+cables+Micro+USB+cabepow&qid=1568807361&s=gateway&sr=8-3[Micro USB Cables]

* Router

=== Software:

* link:/https://www.balena.io/etcher/[BalenaEtcher]

* link:/https://www.docker.com/[Docker]

* link:/https://https://www.raspberrypi.org/downloads/raspbian/[
Raspbian Buster with desktop and recommended software]

[#KandC]
== Understanding Kubernetes

Kubernetes is an <<os, open source>> system, <<co,container orchestration>> tool to manage, scale and automate the deployment of <<ca, containerised applications>>.

Key Terminology:
[#os]
* Open Source 

Open Source refers to a computer program in which the source code is available to the general public, for commercial use, study and modification.

* Cluster

A cluster is a set of computers (Raspberry Pi's) that run containerised applications, managed by Kubernetes. A Kubernetes cluster can have a minimum of one worker node and one master node.

[#po]
* Pods

A pod represents a set of containers running in your cluster.
 This is not to be confused with nodes. A pod runs inside of a node. 


* Replica Set

ReplicaSet ensures that the specified number of pod replicas wanted are running at one time. 

* Images

The instance of a container that holds a set of software needed to run an application.

* Containers

A container is an executable image that contains the software to run the application.

[#co]
* Container Orchestration

Management of containers,  and removing containers to spread application load evenly across host infrastructure. 

* Docker 

Docker is open source software that containerises your applications for you, so you can run it on a Kubernetes cluster. This is what the application sits on. 

* Kubernetes Master

When you want to control the Kubernetes cluster, everything is done through the Master. 




[#KCoRPI]
== Creating a Kubernetes Cluster on a Raspberry Pi

Set up for all Pi's:

//Easiest way to do this is to SSH into all the Pi's, open up four terminal windows and run each command simultaiously.

.1. Flashing OS onto Micro SD cards 
Open balena etcher select the image, click select target image to check whether it is the right MicroSD card, and click flash. Do that four times for each Raspberry Pi.

image::/img/blog/Kube-cluster/balena-etcher.png[align="center"]

.2. Configure all Four Raspberry Pi's 

Set up the configuration for the Raspberry Pi, plug the RPI into a monitor and follow the on screen set up instructions, and configure WIFI connection and when naming the Raspberry Pi's call them Master, Node1, Node2, Node3, as when you SSH into them from your laptop they will be called Master, Node1, Node2, Node3.

//If you do not have a Monitor and keyboard thats fine, you can skip the step of configuring the WIFI from a monitor and do a headless start, so you can SSH into the Raspberry Pi without needing to hook it up. 
 
If you were not prompted or forgot to change the name of the Raspberry Pi, not to worry as you can change the name by typing into the terminal: 

`sudo nano /etc/hostname`

and change the name. Press `ctrl s` to save and `ctrl x` to exit from the nano editor.

`sudo reboot` to see the changes.

.3. Enable SSH on the Raspberry Pi Desktop by going to:
* `Preferences` and select `Raspberry Pi Configuration` 

* `Interfaces` and select `Enabled` next to SSH


.4. Display IP Addresses

Open the Terminal, and type `ifconfig` (InterFace CONFIGurator). It will display the IP address needed to SSH into the Raspberry Pi.

image::/img/blog/Kube-cluster/ifconfig.png[align="center"]

.5. Validating WIFI Network on Raspberry Pi:

Ensure you are on the same WIFI network as the Raspberry Pi, by clicking the WIFI symbol on the rapberry pi OS, and ensure it matches to the WIFI of PC.

image::/img/blog/Kube-cluster/wifi.png[align="left"] image::/img/blog/Kube-cluster/wifimac.png[align="center"]


.7. SSH into the Pi:
`ssh pi@<ip_address>`

`example: ssh pi@192.168.1.118`

image::/img/blog/Kube-cluster/wifi.png[align="left"]

SSH into the Pi, it will ask you if want to trust the authenticity, type yes to trust the computer. Click HERE to learn about authenticity.

image::/img/blog/Kube-cluster/authen.png[align="center"]

If you get errors and cannot connect to the Raspberry Pi. Click HERE to see common issues

Sucessful SSH onto the pi looks like 

image::/img/blog/Kube-cluster/ssh.png[align="center"]

.8. Very Important: Set static IP address' 

When the RPI shuts down the Router will give the RPI a different IP address, and it will not work.

To set static IP on Raspbian Buster:

`sudo nano /etc/dhcpcd.conf` 

`interface eth0 static ip_address=192.168.1.XX/24 static routers=192.168.1.1 static domain_name_servers=192.168.1.1.`


image::/img/blog/Kube-cluster/staticip.png[align="center"]

`sudo reboot`

`sudo apt-get update`

=== Installing Container Runtime Interface (CRI)

In later versions of Kubernetes, v1.6.0 (+), they can automatically detect the Container Runtime Interfaces (CRI) by scanning through a list of well known ports.

.9. Docker

There is a script to install Docker on ARM, for convenience which installs all the dependencies.

`$ curl -fsSL get.docker.com | sh`

If you would like to know what the flag -fsSL is click link:/https://explainshell.com/explain?cmd=curl+-fsSLp[here]


Add the user to Docker, so the user can use Docker

`sudo usermod pi -aG docker`

Test if the script ran successfully by typing 

`docker --version`

image::/img/blog/Kube-cluster/docker-version.png[align="center"]


.10. Disable swapoff
Swapoff disables devices and files for paging and swapping. Kubernetes needs to disable swapoff primarily for performance. if you do not disable swapoff, it will fail the pre-flight checks, and you will not be able to create the Kube-cluster.

`sudo dphys-swapfile swapoff` - Turns swap off

`sudo dphys-swapfile uninstall` - Uninstalls Swap

`sudo update-rc.d dphys-swapfile remove`

`sudo swapon --summary` - Should see nothing

image::/img/blog/Kube-cluster/swapon.png[align="center"]


.11. Edit the boot file

`sudo nano /boot/cmdline.txt`

Add: `cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory`

Add to the end of the line in the file. Do not create a new line.

image::/img/blog/Kube-cluster/cgroups.png[align="center"]

.12. Restart Raspberry Pi

This restarts the Raspberry Pi, with it configured properly.

`sudo reboot`


.13. SSH Back into the Raspberry Pi

Ensure the Swapon is off

`ssh pi@<ip_address>`

`example ssh pi@192.168.0.112`

=== Installing Kubernetes

==== What does Kubeadm, Kubelet and Kubectl do?

*  Kubeadm: Command to bootstrap the cluster. 

This refers to the initial cluster. Bootstrapping means that you are defining which nodes, that should synchronise, and also makes a minimum viable Kubernetes cluster.

* Kubelet: This tool starts pods, containers and runs on all machines.

* Kubectl: This is what you will use most. It is the command to talk to your cluster, such as debugging and retrieving logs.

Ensure Machine is up to date

`apt-get update && apt-get install -y apt-transport-https curl`

Click HERE if you get errors to see the help page

.14. Adding the key

This is downloading the Kubernetes key from a Repository and applying it. If it works, and is applied, the output will be `ok`.

`curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -`

-s means silent. This means it will not show progress bar, or error messages. 

image::/img/blog/Kube-cluster/keyadding.png[align="center"]

.15. Adding the Repository

This opens up the file

`cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF`

image::/img/blog/Kube-cluster/exportkey.png[align="center"]

.16. Retrieve updates

The Kubernetes Repo is added to the list of 'stuff'

`sudo apt-get update`

.17. Installing kubelet kubeadm kubelet

`apt-get install -y kubelet kubeadm kubectl`

Starts the download

We have Kubernetes installed onto one of the Pi's

//.18. Run command 

//`apt-mark hold kubelet kubeadm kubectl`

//If you are unsure about what 'apt-mark' and 'hold' does this is a useful webpage link:/http://manpages.ubuntu.com/manpages/bionic/man8/apt-mark.8.html[here]


//==== Ensure that swapoff is off
//sudo dphys-swapfile swapoff

== Master Node Setup

=== Pod Network add on 

A Pod network add on means that the pods can communicate with eachother. Click here to see definition of <<po,pods>>

This is confusing, and lots of people use different third party add-ons. We are going to use Flannel. 

=== Initialize Kubernetes on master node

ssh into the Master

.18. This creates the Master node, and starts the Kubernetes cluster

`sudo kubeadm init --pod-network-cidr=10.244.0.0/16`


* Cidr - Once we have our Kubernetes master node initialized, we need the pods to communicate to eachother.

//Do not pass through `--token-ttl=0` as the argument as this is bad practice. Ensure that the Pod Network add on has been installed first. 

The successful output looks like:

[source, xml]

[init] Using Kubernetes version: vX.Y.Z
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Activating the kubelet service
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [kubeadm-cp localhost] and IPs [10.138.0.4 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [kubeadm-cp localhost] and IPs [10.138.0.4 127.0.0.1 ::1]
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubeadm-cp kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.138.0.4]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 31.501735 seconds
[uploadconfig] storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-X.Y" in namespace kube-system with the configuration for the kubelets in the cluster
[patchnode] Uploading the CRI Socket information "/var/run/dockershim.sock" to the Node API object "kubeadm-cp" as an annotation
[mark-control-plane] Marking the node kubeadm-cp as control-plane by adding the label "node-role.kubernetes.io/master=''"
[mark-control-plane] Marking the node kubeadm-cp as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: <token>
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstraptoken] creating the "cluster-info" ConfigMap in the "kube-public" namespace
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

    Your Kubernetes control-plane has initialized successfully!

    To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

    You should now deploy a pod network to the cluster.
    Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  /docs/concepts/cluster-administration/addons/

    You can now join any number of machines by running the following on each node
as root:

  kubeadm join <control-plane-host>:<control-plane-port> --token <token> --discovery-token-ca-cert-hash sha256:<hash>


Kubectl needs to work for non root users. 

`mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config`

Click HERE to see what sudo chown $(id -u):$(id -g) $HOME/.kube/config does

=== On the Worker node, joing them to the master node by running the 

Take note, I suggest copying and pasting the following text as it is used to join the worker nodes to the master nodes.

`kubeadm join <control-plane-host>:<control-plane-port> --token <token> --discovery-token-ca-cert-hash sha256:<hash>`

.19. Join worker nodes 

On Node1, Node2, Node3, on the terminal join the worker nodes to the master node. 

Run: `kubeadm join <control-plane-host>:<control-plane-port> --token <token> --discovery-token-ca-cert-hash sha256:<hash>`

=== Set up Pod to Pod Networking

We need Pod to Pod Communication, although we have initialized the Kubernetes cluster, the pods need to talk to eachother. 

If you ran kubectl get nodes, the Nodes will say NotReady, as they cannot comunicate between eachother, as there is no pod to pod networking. 

This is where the third party add on is used to communicate between eachother. 

We are going to use Flannel for our demonstration. Look at the Kubernetes Documentation to see the different types of third party as some have different functionalities and there is not just one answer to .

.20. Install Flannel
`kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml`

Ensure that all the Pods are running 

`watch kubectl get pods --all-namespaces`

Remove taints on the Master. This means that you can schedule pods on it 

`kubectl taint nodes --all node-role.kubernetes.io/master-`


Confirm that you now have nodes in your cluster with the following commands

`kubectl get nodes -o wide`


.19. kubectl get nodes. This will get the connected nodes and the master node. 

NAME{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}STATUS{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}ROLES{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}AGE{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}VERSION

Master{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}Ready{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}master{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}<age>{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}<v_Number>

Node1 {nbsp}{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}Ready{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}worker{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}<age>{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}<v_Number>

Node2 {nbsp}{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}Ready{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}worker{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}<age>{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}<v_Number>

Node3 {nbsp}{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}Ready{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}worker{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}<age>{nbsp}{nbsp}{nbsp}{nbsp}{nbsp}<v_Number>